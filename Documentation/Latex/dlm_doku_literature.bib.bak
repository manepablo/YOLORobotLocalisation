% Encoding: UTF-8

@InProceedings{Rezatofighi1902,
  author    = {Hamid Rezatofighi and Nathan Tsoi JunYoung and Gwak Amir Sadeghian},
  title     = {Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression},
  booktitle = {arXiv:1902.09630v2},
  year      = {2019},
  file      = {:C\:\\DLM\\Documentation\\interception_over_union.pdf:PDF},
}

@InProceedings{Atiqur,
  author    = {Md Atiqur and Rahman and Yang and Wang},
  title     = {Optimizing Intersection-Over-Union in Deep Neural Networks for Image Segmentation},
  booktitle = {Department of Computer Science, University of Manitoba, Canada},
  year      = {2015},
  abstract  = {We consider the problem of learning deep neural networks (DNNs)
for object category segmentation, where the goal is to label each pixel
in an image as being part of a given object (foreground) or not (back-
ground). Deep neural networks are usually trained with simple loss func-
tions (e.g., softmax loss). These loss functions are appropriate for stan-
dard classification problems where the performance is measured by the
overall classification accuracy. For object category segmentation, the two
classes (foreground and background) are very imbalanced. The intersection-
over-union (IoU) is usually used to measure the performance of any ob-
ject category segmentation method. In this paper, we propose an ap-
proach for directly optimizing this IoU measure in deep neural networks.
Our experimental results on two object category segmentation datasets
demonstrate that our approach outperforms DNNs trained with standard
softmax loss.},
  file      = {:C\:\\DLM\\Documentation\\Optimizing_Intersection-Over-Union .pdf:PDF},
}

@InProceedings{Tekin1711,
  author    = {Bugra Tekin and Sudipta N. Sinha and Pascal Fua and EPFL Microsoft and Research EPFL},
  title     = {Real-Time Seamless Single Shot 6D Object Pose Prediction},
  booktitle = {arXiv:1711.08848v5},
  year      = {2018},
  abstract  = {the object and another to predict the 2D locations of the
projections of the object’s 3D bounding box given the seg-},
  file      = {:C\:\\DLM\\Documentation\\1711.08848.pdf:PDF},
}

@InProceedings{Tremblay1809,
  author    = {Jonathan Tremblay and Thang To and Balakumar Sundaralingam},
  title     = {Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects},
  booktitle = {arXiv:1809.10790v1},
  year      = {2018},
  abstract  = {Using synthetic data for training deep neural networks for robotic manipulation
holds the promise of an almost unlimited amount of pre-labeled training data,
generated safely out of harm’s way. One of the key challenges of synthetic data,
to date, has been to bridge the so-called reality gap, so that networks trained on
synthetic data operate correctly when exposed to real-world data. We explore the
reality gap in the context of 6-DoF pose estimation of known objects from a single
RGB image. We show that for this problem the reality gap can be successfully
spanned by a simple combination of domain randomized and photorealistic data.
Using synthetic data generated in this manner, we introduce a one-shot deep neural
network that is able to perform competitively against a state-of-the-art network
trained on a combination of real and synthetic data. To our knowledge, this is the
first deep network trained only on synthetic data that is able to achieve state-of-the-
art performance on 6-DoF object pose estimation. Our network also generalizes
better to novel environments including extreme lighting conditions, for which we
show qualitative results. Using this network we demonstrate a real-time system
estimating object poses with sufficient accuracy for real-world semantic grasping
of known household objects in clutter by a real robot.1},
  file      = {:C\:\\DLM\\Documentation\\1809.10790.pdf:PDF},
  keywords  = {computer vision, pose estimation, synthetic data, randomization},
}

@InProceedings{Surgery1803,
  author    = {Robot-Assisted Surgery and Using Deep and Learning},
  title     = {Automatic Instrument Segmentation in},
  booktitle = {arXiv:1803.01207v2},
  year      = {2018},
  abstract  = {Semantic segmentation of robotic instruments is an impor-},
  file      = {:C\:\\DLM\\Documentation\\machine_learning_robot_surgery.pdf:PDF},
  keywords  = {Medical imaging, Robot-assisted surgery, Computer vision, Image segmentation, Deep learning},
}

@InProceedings{Abstract2016,
  author    = {Muhammad Imran Razzak, Saeeda Naz, Ahmad Zaib},
  title     = {Deep Learning for Medical Image Processing: Overview, Challenges and Future},
  booktitle = {CPHHI},
  year      = {2016},
  file      = {:C\:\\DLM\\Documentation\\dlinmedicine.pdf:PDF},
}

@InProceedings{Ouyang2014,
  author    = {Wanli Ouyang and Xiaogang Wang and Xingyu Zeng and Shi Qiu and Ping Luo and Yonglong Tian and Hongsheng Li and Shuo Yang and Zhe Wang and Chen-Change Loy and Xiaoou Tang},
  title     = {DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection},
  booktitle = {CVPR 2015 p.2403-2412},
  year      = {2014},
  abstract  = {Pretrained on object-level annoation Pretrained on image-level annotation},
  file      = {:C\:\\DLM\\Documentation\\Ouyang_DeepID-Net_Deformable_Deep_2015_CVPR_paper.pdf:PDF},
}

@Article{Cruz-Roa2017,
  author    = {Angel Cruz-Roa and Hannah Gilmore and Ajay Basavanhally and Michael Feldman and Shridar Ganesan and Natalie N.C. Shih and John Tomaszewski and Fabio A. Gonz{\'{a}}lez and Anant Madabhushi},
  title     = {Accurate and reproducible invasive breast cancer detection in whole-slide images: A Deep Learning approach for quantifying tumor extent},
  journal   = {Scientific Reports},
  year      = {2017},
  volume    = {7},
  number    = {1},
  month     = {apr},
  doi       = {10.1038/srep46450},
  file      = {:C\:\\DLM\\Documentation\\Brest_cancer_detection.pdf:PDF},
  publisher = {Springer Science and Business Media {LLC}},
}

@Book{dlbook2018,
  title     = {Deep Learning. Das umfassende Handbuch: Grundlagen, aktuelle Verfahren und Algorithmen, neue Forschungsansätze (mitp Professional)},
  publisher = {mitp Professionals},
  year      = {2018},
  author    = {Ian Goodfellow, Yoshua Bengio, Aaron Courville},
  isbn      = {3958457002},
}

@Book{dlazure2019,
  title     = {Deep Learning mit Microsoft Azure},
  publisher = {Rheinwerk Computing},
  year      = {2019},
  author    = {Mathew Salvaris, Danielle Dean, Wee Hyong Tok},
  editor    = {Dr. Christoph Meister},
}

@Book{Goodfellow-et-al-2016,
  title     = {Deep Learning},
  publisher = {MIT Press},
  year      = {2016},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  note      = {\url{http://www.deeplearningbook.org}},
}

@InProceedings{Redmon2016,
  author    = {Joseph Redmon and Santosh Divvala and Ross Girshick and Ali Farhadi},
  title     = {You Only Look Once: Unified, Real-Time Object Detection},
  booktitle = {arXiv:1506.02640v5},
  year      = {2016},
  abstract  = {Person: 0.64},
  file      = {:C\:\\DLM\\Documentation\\yolo.pdf:PDF},
}

@InProceedings{Detection2019,
  author    = {Zhong-Qiu Zhao and Peng Zheng and Shou-tao Xu and Xindong Wu},
  title     = {Object Detection with Deep Learning: A Review},
  booktitle = {arXiv:1807.05511v2},
  year      = {2019},
  publisher = {IEEE},
  abstract  = {Due to object detection’s close relationship with object localization task. So much attention has been attracted
video analysis and image understanding, it has attracted much to this field in recent years [15]–[18].
research attention in recent years. Traditional object detection The problem definition of object detection is to determine
methods are built on handcrafted features and shallow trainable
architectures. Their performance easily stagnates by constructing where objects are located in a given image (object localization)
complex ensembles which combine multiple low-level image and which category each object belongs to (object classifica-
features with high-level context from object detectors and scene tion). So the pipeline of traditional object detection models
classifiers. With the rapid development in deep learning, more can be mainly divided into three stages: informative region
powerful tools, which are able to learn semantic, high-level, selection, feature extraction and classification.
deeper features, are introduced to address the problems existing
in traditional architectures. These models behave differently Informative region selection. As different objects may appear
in network architecture, training strategy and optimization in any positions of the image and have different aspect ratios
function, etc. In this paper, we provide a review on deep or sizes, it is a natural choice to scan the whole image with a
learning based object detection frameworks. Our review begins multi-scale sliding window. Although this exhaustive strategy
with a brief introduction on the history of deep learning and can find out all possible positions of the objects, its short-
its representative tool, namely Convolutional Neural Network
(CNN). Then we focus on typical generic object detection comings are also obvious. Due to a large number of candidate
architectures along with some modifications and useful tricks windows, it is computationally expensive and produces too
to improve detection performance further. As distinct specific many redundant windows. However, if only a fixed number of
detection tasks exhibit different characteristics, we also briefly sliding window templates are applied, unsatisfactory regions
survey several specific tasks, including salient object detection, may be produced.
face detection and pedestrian detection. Experimental analyses
are also provided to compare various methods and draw some Feature extraction. To recognize different objects, we need
meaningful conclusions. Finally, several promising directions and to extract visual features which can provide a semantic and
tasks are provided to serve as guidelines for future work in robust representation. SIFT [19], HOG [20] and Haar-like [21]
both object detection and relevant neural network based learning features are the representative ones. This is due to the fact
systems. that these features can produce representations associated with},
  file      = {:C\:\\DLM\\Documentation\\object_detection_dl_uebersicht_.pdf:PDF},
}

@InProceedings{Liu2016,
  author    = {Wei Liu and Dragomir Anguelov and Dumitru Erhan and Christian Szegedy and Scott Reed and Cheng-Yang Fu and Alexander C. Berg},
  title     = {SSD: Single Shot MultiBox Detector},
  booktitle = {arXiv:1512.02325v5},
  year      = {2016},
  abstract  = {We present a method for detecting objects in images using a single
deep neural network. Our approach, named SSD, discretizes the output space of
bounding boxes into a set of default boxes over different aspect ratios and scales
per feature map location. At prediction time, the network generates scores for the
presence of each object category in each default box and produces adjustments to
the box to better match the object shape. Additionally, the network combines pre-
dictions from multiple feature maps with different resolutions to naturally handle
objects of various sizes. SSD is simple relative to methods that require object
proposals because it completely eliminates proposal generation and subsequent
pixel or feature resampling stages and encapsulates all computation in a single
network. This makes SSD easy to train and straightforward to integrate into sys-
tems that require a detection component. Experimental results on the PASCAL
VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy
to methods that utilize an additional object proposal step and is much faster, while
providing a unified framework for both training and inference. For 300× 300 in-
put, SSD achieves 74.3% mAP1 on VOC2007 test at 59 FPS on a Nvidia Titan
X and for 512 × 512 input, SSD achieves 76.9% mAP, outperforming a compa-
rable state-of-the-art Faster R-CNN model. Compared to other single stage meth-
ods, SSD has much better accuracy even with a smaller input image size. Code is
available at: https://github.com/weiliu89/caffe/tree/ssd .},
  file      = {:C\:\\DLM\\Documentation\\ssd.pdf:PDF},
  keywords  = {Real-time Object Detection; Convolutional Neural Network},
}

@InProceedings{Girshick2015,
  author    = {Ross Girshick and Microsoft Research and rbg@microsoft.com},
  title     = {Fast R-CNN},
  booktitle = {arXiv:1504.08083v2},
  year      = {2015},
  abstract  = {while achieving top accuracy on PASCAL VOC 2012 [7]
with a mAP of 66% (vs. 62% for R-CNN).1},
  file      = {:C\:\\DLM\\Documentation\\f_rcnn.pdf:PDF},
}

@Misc{chollet2015keras,
  author       = {Chollet, Fran\c{c}ois and others},
  title        = {Keras},
  howpublished = {\url{https://keras.io}},
  year         = {2015},
}

@Article{Xu2019,
  author    = {Jun Xu and Yanxin Ma and Songhua He and Jiahua Zhu},
  title     = {3D-{GIoU}: 3D Generalized Intersection over Union for Object Detection in Point Cloud},
  journal   = {Sensors MDPI},
  year      = {2019},
  volume    = {19},
  number    = {19},
  pages     = {4093},
  month     = {sep},
  doi       = {10.3390/s19194093},
  file      = {:C\:\\DLM\\Documentation\\3D-GIoU_3D_Generalized_Intersection_over_Union_for.pdf:PDF},
  publisher = {{MDPI} {AG}},
}

@InProceedings{Mousavian1612,
  author    = {Arsalan Mousavian and Dragomir Anguelov and John Flynn and George Mason University Zoox and Inc. Zoox and Inc},
  title     = {3D Bounding Box Estimation Using Deep Learning and Geometry},
  booktitle = {arXiv:1612.00496v2},
  year      = {2017},
  file      = {:C\:\\DLM\\Documentation\\loss_3d_3vorschlaege.pdf:PDF},
}

@Comment{jabref-meta: databaseType:bibtex;}
